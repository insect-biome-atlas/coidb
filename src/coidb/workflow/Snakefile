import polars as pl
import pandas as pd
import string
import gzip as gz
from tempfile import NamedTemporaryFile
from snakemake.utils import validate
import os
import sys

validate(config, "schemas/config.schema.yaml")

if not config["input_file"]:
    sys.stderr.write("NO INPUT FILE SPECIFIED! Use --input_file flag to set.\n")
    sys.exit(1)

from coidb import series_to_fasta


localrules:
    extract_input,
    gzip,
    collect_vsearch,
    download_backbone,
    concat_timestamps


rule all:
    input:
        expand(
            "{output_dir}/{f}",
            output_dir=config["output_dir"],
            f=[
                "coidb.dada2.toGenus.fasta.gz",
                "coidb.dada2.toSpecies.fasta.gz",
                "coidb.dada2.addSpecies.fasta.gz",
                "coidb.sintax.fasta.gz",
                "coidb.BOLD_BIN.consensus_taxonomy.tsv.gz",
                "from_backbone.BOLD_BIN.consensus_taxonomy.tsv.gz",
                "coidb.info.tsv.gz",
                "timestamps.txt"
            ]
        )

rule download_backbone:
    message: "Downloading GBIF taxonomy backbone"
    output:
        tsv=expand("{output_dir}/_backbone/Taxon.tsv", output_dir=config["output_dir"]),
        timestamp=temp(expand("{output_dir}/backbone_timestamp.txt", output_dir=config["output_dir"]))
    log:
        os.path.join(config["output_dir"], "_logs/download_backbone.log"),
    params:
        output_dir = lambda wildcards, output: os.path.dirname(output[0]),
        url = "https://hosted-datasets.gbif.org/datasets/backbone/current/backbone.zip"
    shell:
        """
        curl -o {params.output_dir}/backbone.zip {params.url} > {log} 2>&1
        echo "GBIF backbone: " > {output.timestamp}
        timestamp=$(unzip -p {params.output_dir}/backbone.zip eml.xml | grep dateStamp | cut -f2 -d '>' | cut -f1 -d '<')
        echo "  - date: $timestamp" >> {output.timestamp}
        echo "  - url: {params.url}" >> {output.timestamp}
        unzip -p {params.output_dir}/backbone.zip Taxon.tsv | cut -f1,3,6,8,12,18- > {output.tsv}
        rm {params.output_dir}/backbone.zip
        """

rule parse_backbone:
    message: "Extracting backbone taxonomy for BOLD bins"
    output:
        temp(expand("{output_dir}/_gbif-backbone.bold_bin.taxonomy.tsv", output_dir=config["output_dir"]))
    input:
        rules.download_backbone.output[0]
    log:
        os.path.join(config["output_dir"], "_logs/parse_backbone.log")
    shell:
        """
        parse-backbone {input} {output} 2>{log}
        """

rule fill_missing_backbone:
    message: "Filling missing ranks for backbone taxonomy"
    output:
        expand("{output_dir}/gbif-backbone.bold_bin.taxonomy.tsv", output_dir=config["output_dir"])
    input:
        rules.parse_backbone.output[0]
    log:
        os.path.join(config["output_dir"], "_logs/fill_missing_backbone.log")
    shell:
        """
        fill-missing -i {input} -o {output} 2>{log}
        """


rule gzip:
    message: "Compressing {wildcards.f}"
    input:
        "{f}.tsv"
    output:
        "{f}.tsv.gz"
    shell:
        """
        gzip -c {input} > {output}
        """

checkpoint extract_input:
    message: "Extracting input file {config[input_file]}"
    input:
        tarball=config["input_file"],
    output:
        directory(os.path.join(config["output_dir"],"_extract")),
    log:
        os.path.join(config["output_dir"], "_logs/extract_input.log"),
    shell:
        """
        rm -rf {output}
        mkdir -p {output}
        tar -C {output} -xvf {input.tarball} > {log} 2>&1
        """


def get_extracted(wildcards):
    if config["input_file"].endswith(".tar.gz"):
        checkpoint_output = checkpoints.extract_input.get(**wildcards).output[0]
        tsv = expand(
            "{output_dir}/_extract/{f}.tsv",
            f=glob_wildcards(os.path.join(checkpoint_output, "{f}.tsv")).f,
            output_dir=config["output_dir"]
        )
    elif config["input_file"].endswith(".tsv"):
        tsv = config["input_file"]
    return tsv


rule filter:
    message:
        "Filter data to only include BOLD BIN records, trim and remove spurious sequences"
    input:
        tsv=get_extracted,
    output:
        tsv=os.path.join(config["output_dir"], "_processed/01-data.filtered.tsv"),
        timestamp=temp(expand("{output_dir}/input_timestamp.txt", output_dir=config["output_dir"]))
    log:
        os.path.join(config["output_dir"], "_logs/filter.log"),
    params:
        min_len=config["min_len"],
        input_file=config["input_file"]
    resources:
        mem_mb = 12000
    shell:
        """
        echo "Input data:" >> {output.timestamp}
        echo "  - filename: {params.input_file}" >> {output.timestamp}
        echo "  - extracted filename: {input.tsv}" >> {output.timestamp}
        filter-records -i {input.tsv} -o {output.tsv} -l {params.min_len} 2>{log}
        """


checkpoint fill_missing:
    message:
        "Filling missing ranks"
    input:
        tsv=rules.filter.output.tsv,
    output:
        tsv=temp(os.path.join(config["output_dir"], "coidb.info.tsv")),
    log:
        os.path.join(config["output_dir"], "_logs/fill_missing.log"),
    shell:
        """
        fill-missing -i {input.tsv} -o {output.tsv} 2>{log}
        """


rule calculate_consensus:
    message:
        "Calculating consensus taxonomy for BINs"
    input:
        tsv=rules.fill_missing.output.tsv,
        backbone=rules.fill_missing_backbone.output[0]
    output:
        tsv=temp(os.path.join(config["output_dir"], "coidb.BOLD_BIN.consensus_taxonomy.tsv")),
        existing=temp(os.path.join(config["output_dir"], "from_backbone.BOLD_BIN.consensus_taxonomy.tsv"))
    log:
        os.path.join(config["output_dir"], "_logs/calculate_consensus.log"),
    params:
        threshold=config["consensus_threshold"],
        ranks=config["ranks"],
        method=config["consensus_method"]
    threads: workflow.cores
    shell:
        """
        consensus-taxonomy -i {input.tsv} -o {output.tsv} -u {input.backbone} --write_existing {output.existing} -r {params.ranks} -p {threads} -t {params.threshold} 2>{log}
        """


rule vsearch:
    message:
        "Clustering sequences in {wildcards.bin_uri}"
    input:
        tsv=rules.fill_missing.output.tsv,
    output:
        fasta=temp(
                expand(
                    "{output_dir}/_vsearch/{{bin_uri}}.fasta",
                    output_dir=config["output_dir"]
                )
            ),
    log:
        expand(
            "{output_dir}/_logs/vsearch_logs/{{bin_uri}}.log",
            output_dir=config["output_dir"]
        )
    threads: 1
    params:
        id=config["vsearch_identity"],
    group:
        "vsearch"
    resources:
        runtime=20,
    run:
        # Create a temporary file to hold the sequences for vsearch
        f = NamedTemporaryFile(mode="w", delete=False)
        # Extract a subset of the dataframe for this BOLD BIN
        df = (
            pl.scan_csv(input.tsv, separator="\t")
            .filter(pl.col("bin_uri") == wildcards.bin_uri)
            .collect()
        )
        # Write a fasta file with the process ids as headers
        fasta_series = ">" + df["processid"] + "\n" + df["seq"]
        series_to_fasta(fasta_series, f.name)
        # Run vsearch on the temporary fasta
        shell(
            f"vsearch --cluster_fast {f.name} --id {params.id} --centroids {output.fasta} --notrunclabels --threads {threads} 2>{log}"
        )


def get_vsearch_fastas(wildcards):
    """
    This function reads the dataframe of all records and finds BOLD BINs with
    more than one record -> multi_record_bins. Sequences in these BOLD BINs will
    be clustered using vsearch, so a list of input files is generated and
    returned.
    """
    # Get the TSV file from the fill_missing checkpoint
    checkpoint_output = checkpoints.fill_missing.get(**wildcards).output.tsv
    # Read the dataframe with lazy API and filter to only BOLD BINs with more than one record
    q = (
        pl.scan_csv(checkpoint_output, separator="\t")
        .group_by("bin_uri")
        .len()
        .filter(pl.col("len") > 1)
    ).collect()
    # multi_record_bins is a list of all BOLD BINs with more than 1 record
    multi_record_bins = q["bin_uri"].to_list()
    # Return a list of vsearch output files which will be used as targets for the collect_vsearch rule,
    # thus triggering vsearch to run on these BOLD BINs
    return expand("{output_dir}/_vsearch/{bin_uri}.fasta", bin_uri=multi_record_bins, output_dir=config["output_dir"])


rule collect_vsearch:
    message:
        "Collect vsearch output"
    input:
        fasta=get_vsearch_fastas,
    output:
        fasta=temp(os.path.join(config["output_dir"], "_processed/04-data.vsearch_clustered.fasta.gz")),
    log:
        os.path.join(config["output_dir"], "_logs/collect_vsearch.log"),
    shell:
        """
        cat {input.fasta} > {output.fasta} 2>{log}
        """


rule add_to_vsearch:
    message:
        "Adding records to vsearch output"
    input:
        tsv=rules.fill_missing.output.tsv,
        fasta=rules.collect_vsearch.output.fasta,
    output:
        fasta=os.path.join(config["output_dir"], "coidb.clustered.fasta.gz"),
    run:
        # Collect all BOLD BINs with just 1 record
        q = (
            pl.scan_csv(input.tsv, separator="\t")
            .group_by("bin_uri")
            .len()
            .filter(pl.col("len") == 1)
        ).collect()
        single_record_bins = q["bin_uri"].to_list()
        # Read rows matching only the single_record_bins
        df = (
            pl.scan_csv(input.tsv, separator="\t").filter(
                pl.col("bin_uri").is_in(single_record_bins)
            )
        ).collect()
        # Write records to a temporary file
        f = NamedTemporaryFile(mode="w", delete=False)
        fasta_series = ">" + df["processid"] + "\n" + df["seq"]
        series_to_fasta(fasta_series, f.name)
        # Concatenate single record bins fasta + vsearch clustered fasta
        shell("cat {f.name} {input.fasta} | gzip -c > {output.fasta}")


rule format_sintax:
    message:
        "Generate SINTAX format fasta file"
    input:
        fasta=rules.add_to_vsearch.output.fasta,
        tsv=rules.fill_missing.output.tsv,
        consensus=rules.calculate_consensus.output.tsv,
    output:
        fasta=os.path.join(config["output_dir"], "coidb.sintax.fasta.gz"),
    log:
        os.path.join(config["output_dir"], "_logs/format_sintax.log"),
    shell:
        """
        format-fasta --tsv {input.tsv} --fasta {input.fasta} --consensus {input.consensus} sintax --outfile {output} 2>{log}
        """


rule format_dada2:
    message:
        "Generate DADA2 format fasta files"
    input:
        fasta=rules.add_to_vsearch.output.fasta,
        tsv=rules.fill_missing.output.tsv,
        consensus=rules.calculate_consensus.output.tsv,
    output:
        to_genus=os.path.join(config["output_dir"], "coidb.dada2.toGenus.fasta.gz"),
        to_species=os.path.join(config["output_dir"], "coidb.dada2.toSpecies.fasta.gz"),
        assign_species=os.path.join(config["output_dir"], "coidb.dada2.addSpecies.fasta.gz"),
    log:
        os.path.join(config["output_dir"], "_logs/format_dada2.log"),
    shell:
        """
        format-fasta --tsv {input.tsv} --fasta {input.fasta} --consensus {input.consensus} dada2 \
            --outfile_toGenus {output.to_genus} --outfile_toSpecies {output.to_species} --outfile_assignSpecies {output.assign_species} 2>{log}
        """

rule concat_timestamps:
    output:
        os.path.join(config["output_dir"], "timestamps.txt")
    input:
        rules.download_backbone.output.timestamp,
        rules.filter.output.timestamp
    shell:
        """
        cat {input} > {output}
        """
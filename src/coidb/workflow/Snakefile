import polars as pl
import pandas as pd
import string
import gzip as gz
from tempfile import NamedTemporaryFile
from snakemake.utils import validate
import os
import sys

validate(config, "schemas/config.schema.yaml")

if not config["input_file"]:
    sys.stderr.write("NO INPUT FILE SPECIFIED! Use --input_file flag to set.\n")
    sys.exit(1)

if not config["temp_dir"]:
    config["temp_dir"] = os.path.join(config["output_dir"], "tmp")

from coidb import series_to_fasta


localrules:
    extract_input,
    gzip,
    collect_vsearch,
    download_backbone,
    concat_timestamps

wildcard_constraints:
    n = "backbone|coidb"


rule all:
    input:
        expand(
            "{output_dir}/{f}",
            output_dir=config["output_dir"],
            f=[
                "dada2/coidb.dada2.toGenus.fasta.gz",
                "dada2/coidb.dada2.toSpecies.fasta.gz",
                "dada2/coidb.dada2.addSpecies.fasta.gz",
                "sintax/coidb.sintax.fasta.gz",
                "qiime2/coidb.qiime2.info.tsv.gz",
                "coidb.BOLD_BIN.consensus_taxonomy.tsv.gz",
                "backbone.info.tsv.gz",
                "from_backbone.BOLD_BIN.consensus_taxonomy.tsv.gz",
                "coidb.info.tsv.gz",
                "timestamps.txt"
            ]
        )

rule download_backbone:
    """
    This rule downloads the latest GBIF backbone release.
    """
    message: "Downloading GBIF taxonomy backbone"
    output:
        tsv=temp(expand("{tempdir}/_backbone/Taxon.tsv", tempdir=config["temp_dir"])),
        timestamp=temp(expand("{output_dir}/backbone_timestamp.txt", output_dir=config["output_dir"]))
    log:
        os.path.join(config["output_dir"], "_logs/download_backbone.log"),
    params:
        output_dir = lambda wildcards, output: os.path.dirname(output[0]),
        url = "https://hosted-datasets.gbif.org/datasets/backbone/current/backbone.zip"
    shell:
        """
        curl -o {params.output_dir}/backbone.zip {params.url} > {log} 2>&1
        echo "GBIF backbone: " > {output.timestamp}
        timestamp=$(unzip -p {params.output_dir}/backbone.zip eml.xml | grep dateStamp | cut -f2 -d '>' | cut -f1 -d '<')
        echo "  - date: $timestamp" >> {output.timestamp}
        echo "  - url: {params.url}" >> {output.timestamp}
        unzip -p {params.output_dir}/backbone.zip Taxon.tsv | cut -f1,3,6,8,12,18- > {output.tsv}
        rm {params.output_dir}/backbone.zip
        """

rule parse_backbone:
    """
    This rule parses the GBIF backbone for downstream use.
    """
    message: "Extracting backbone taxonomy for BOLD bins"
    output:
        temp(expand("{tempdir}/_backbone/bold_bin.taxonomy.tsv", tempdir=config["temp_dir"]))
    input:
        rules.download_backbone.output[0]
    log:
        os.path.join(config["output_dir"], "_logs/parse_backbone.log")
    shell:
        """
        parse-backbone {input} {output} 2>{log}
        """


rule gzip:
    message: "Compressing {wildcards.f}"
    input:
        "{f}.tsv"
    output:
        "{f}.tsv.gz"
    shell:
        """
        gzip -c {input} > {output}
        """

checkpoint extract_input:
    """
    This checkpoint extracts the input tarball (if specified).
    """
    message: "Extracting input file {config[input_file]}"
    input:
        tarball=config["input_file"],
    output:
        directory(os.path.join(config["temp_dir"],"_extract")),
    log:
        os.path.join(config["output_dir"], "_logs/extract_input.log"),
    shell:
        """
        rm -rf {output}
        mkdir -p {output}
        tar -C {output} -xvf {input.tarball} > {log} 2>&1
        """


def get_extracted(wildcards):
    """
    This function identifies the filename of the *.tsv file contained in the
    input tarball.
    """
    if config["input_file"].endswith(".tar.gz"):
        checkpoint_output = checkpoints.extract_input.get(**wildcards).output[0]
        tsv = expand(
            "{temp_dir}/_extract/{f}.tsv",
            f=glob_wildcards(os.path.join(checkpoint_output, "{f}.tsv")).f,
            temp_dir=config["temp_dir"]
        )
    elif config["input_file"].endswith(".tsv"):
        tsv = config["input_file"]
    return tsv


rule filter:
    """
    This rule filters the input data by:
    1. Selecting a subset of columns
    2. Only keeping records matching COI-5P as 'marker_code'
    3. Only keeping records with an assigned BOLD BIN
    4. Only keeping records with a sequence length >= min_len (default 500)
    5. Strips leading and trailing '-' characters
    6. Removes sequences with with remaining gaps
    7. Removes sequences with non DNA characters

    For prokaryotic records lines are kept in step 3 even if no BOLD BIN is
    assigned.
    """
    message:
        "Filter data to only include BOLD BIN records, trim and remove spurious sequences"
    input:
        tsv=get_extracted,
    output:
        tsv=temp(os.path.join(config["temp_dir"], "_processed/data.filtered.tsv")),
        timestamp=temp(expand("{output_dir}/input_timestamp.txt", output_dir=config["output_dir"]))
    log:
        os.path.join(config["output_dir"], "_logs/filter.log"),
    params:
        min_len=config["min_len"],
        input_file=config["input_file"]
    shell:
        """
        echo "Input data:" >> {output.timestamp}
        echo "  - filename: {params.input_file}" >> {output.timestamp}
        echo "  - extracted filename: {input.tsv}" >> {output.timestamp}
        filter-records -i {input.tsv} -o {output.tsv} -l {params.min_len} 2>{log}
        """

def fill_missing_input(wildcards):
    if wildcards.n == "backbone":
        return rules.parse_backbone.output[0]
    elif wildcards.n == "coidb":
        return rules.filter.output.tsv

rule fill_missing:
    """
    This rule fills in missing ranks in taxonomic dataframes.
    """
    message:
        "Filling missing ranks for file {wildcards.n}.info.tsv"
    input:
        fill_missing_input
    output:
        tsv=temp(expand("{output_dir}/{{n}}.info.tsv", output_dir=config["output_dir"]))
    log:
        os.path.join(config["output_dir"], "_logs/fill_missing.{n}.log"),
    shell:
        """
        fill-missing -i {input} -o {output.tsv} 2>{log}
        """

checkpoint multi_record_bins:
    """
    BINs with more than 1 record are subject to clustering with vsearch. This
    rule identifies all such BINs and writes these to a textfile. It also
    outputs a fasta file of all records in these bins. The header in this fasta
    file contains the BIN id for lookup with seqkit grep. The header has the
    format:

    ><processid> bin_uri:<bin_uri>
    
    The fasta file is also indexed with seqkit faidx.
    """
    message:
        "Identifying multi-record BINs"
    input:
        tsv=os.path.join(config["output_dir"], "coidb.info.tsv")
    output:
        fasta=temp(os.path.join(config["temp_dir"], "_processed/multi_record_bins.fasta")),
        fai=temp(os.path.join(config["temp_dir"], "_processed/multi_record_bins.fasta.fai")),
        tsv=temp(os.path.join(config["temp_dir"], "_processed/multi_record_bins.tsv"))
    log:
        os.path.join(config["output_dir"], "_logs/multi_record_bins.log"),
    run:
        df = (pl.scan_csv(input.tsv, separator="\t", low_memory=True).select(["processid","bin_uri","seq"]))
        # Collect all BOLD BINs with > 1 record
        q = (
            df
            .filter(pl.col("bin_uri").str.starts_with("BOLD:"))
            .group_by("bin_uri")
            .len()
            .filter(pl.col("len") > 1)
            .select("bin_uri")
        ).collect(engine="streaming")
        # Write textfile
        q.write_csv(output.tsv, separator="\t")
        multi_record_bins = q.to_series().to_list()
        # Write sequences to file
        df.filter(
            pl.col("bin_uri").is_in(multi_record_bins)
        ).with_columns(
            s=">"+pl.col("processid")+" bin_uri:"+pl.col("bin_uri")+"\n"+pl.col("seq")
        ).select("s").sink_csv(output.fasta, include_header=False, quote_style="never")
        # Generate fasta index
        shell(
            "seqkit faidx {output.fasta} 2>{log}"
        )


rule calculate_consensus:
    """
    This rule calculates a consensus taxonomy for BOLD BINs. It uses as input
    both the BOLD data and the GBIF backbone. If BINs are present in the
    backbone this information takes precedence.
    """
    message:
        "Calculating consensus taxonomy for BINs"
    input:
        tsv=os.path.join(config["output_dir"], "coidb.info.tsv"),
        backbone=os.path.join(config["output_dir"], "backbone.info.tsv")
    output:
        tsv=temp(os.path.join(config["output_dir"], "coidb.BOLD_BIN.consensus_taxonomy.tsv")),
        existing=temp(os.path.join(config["output_dir"], "from_backbone.BOLD_BIN.consensus_taxonomy.tsv"))
    log:
        os.path.join(config["output_dir"], "_logs/calculate_consensus.log"),
    params:
        threshold=config["consensus_threshold"],
        ranks=config["ranks"],
        method=config["consensus_method"]
    threads: 1
    shell:
        """
        consensus-taxonomy -i {input.tsv} -o {output.tsv} -u {input.backbone} --write_existing {output.existing} -r {params.ranks} -p {threads} -t {params.threshold} 2>{log}
        """

rule vsearch:
    """
    This rule clusters sequences for each BOLD BIN.
    """
    message:
        "Clustering sequences in {wildcards.bin_uri}"
    input:
        fasta=os.path.join(config["temp_dir"], "_processed/multi_record_bins.fasta"),
        fai=os.path.join(config["temp_dir"], "_processed/multi_record_bins.fasta.fai"),
    output:
        fasta=temp(
                expand(
                    "{tempdir}/_vsearch/{{bin_uri}}.fasta",
                    tempdir=config["temp_dir"]
                )
            ),
    log:
        expand(
            "{tempdir}/_logs/vsearch_logs/{{bin_uri}}.log",
            tempdir=config["temp_dir"]
        )
    threads: 1
    params:
        id=config["vsearch_identity"],
    group:
        "vsearch"
    run:
        # Create a temporary file to hold the sequences for vsearch
        f = NamedTemporaryFile(mode="w", delete=False)
        # Extract sequences matching wildcards.bin_uri
        shell(
            f"seqkit grep -r -n -p bin_uri:{wildcards.bin_uri} -o {f.name} {input.fasta}"
        )
        # Run vsearch on the temporary fasta
        shell(
            f"vsearch --cluster_fast {f.name} --id {params.id} --centroids {output.fasta} --threads {threads} 2>{log}"
        )
        os.remove(f.name)


def get_vsearch_fastas(wildcards):
    """
    This function reads the multi_record_bins.tsv file generated by the seqs_df
    checkpoint and generates a list of multi_record bins. This list is then used
    to generate input files for vsearch.
    """
    # Get the tsv file containing multi record bins
    checkpoint_output = checkpoints.multi_record_bins.get(**wildcards).output.tsv
    df = pl.read_csv(checkpoint_output, separator="\t")
    # Generate a list of BIN ids
    multi_record_bins = df.select("bin_uri").to_series().to_list()
    # Return a list of vsearch output files which will be used as targets for the collect_vsearch rule,
    # thus triggering vsearch to run on these BOLD BINs
    return expand("{tempdir}/_vsearch/{bin_uri}.fasta", bin_uri=multi_record_bins, tempdir=config["temp_dir"])


rule collect_vsearch:
    """
    This rule concatenates all the vsearch clustered fasta files.
    """
    message:
        "Collect vsearch output"
    input:
        fasta=get_vsearch_fastas,
    output:
        fasta=temp(os.path.join(config["temp_dir"], "_processed/vsearch_clustered.fasta.gz")),
    log:
        os.path.join(config["output_dir"], "_logs/collect_vsearch.log"),
    shell:
        """
        cat {input.fasta} > {output.fasta} 2>{log}
        """


rule add_to_vsearch:
    """
    This rule adds sequences from single-record BINs to the vsearch clustered
    fasta file.
    """
    message:
        "Adding records to vsearch output"
    input:
        tsv=os.path.join(config["output_dir"], "coidb.info.tsv"),
        fasta=rules.collect_vsearch.output.fasta,
    output:
        fasta=os.path.join(config["output_dir"], "coidb.clustered.fasta.gz"),
    run:
        df = pl.scan_csv(input.tsv, separator="\t", low_memory=True).select(["processid","bin_uri","seq"])
        # Collect all BOLD BINs with just 1 record
        q = (
            df
            .group_by("bin_uri")
            .len()
            .filter(pl.col("len") == 1)
            .select("bin_uri")
        ).collect(engine="streaming")
        single_record_bins = q["bin_uri"].to_list()
        # Get records for these bins and write seqs to temporary file
        f = NamedTemporaryFile(mode="w", delete=False)
        df.filter(
            pl.col("bin_uri").is_in(single_record_bins)
        ).with_columns(
            s=">"+pl.col("processid")+"\n"+pl.col("seq")
        ).select("s").sink_csv(f.name, include_header=False, quote_style="never")
        # Concatenate single record bins fasta + vsearch clustered fasta
        shell("cat {f.name} {input.fasta} | seqkit seq -u | gzip -c > {output.fasta}")
        os.remove(f.name)


rule format_sintax:
    """
    This rule formats the vsearch clustered fasta into a SINTAX compatible
    format.
    """
    message:
        "Generate SINTAX format fasta file"
    input:
        fasta=rules.add_to_vsearch.output.fasta,
        tsv=os.path.join(config["output_dir"], "coidb.info.tsv"),
        consensus=rules.calculate_consensus.output.tsv,
    output:
        fasta=temp(os.path.join(config["output_dir"], "sintax/coidb.sintax.fasta.gz")),
    log:
        os.path.join(config["output_dir"], "_logs/format_sintax.log"),
    shell:
        """
        format --tsv {input.tsv} --fasta {input.fasta} --consensus {input.consensus} \
            sintax --outfile {output.fasta} 2>{log}
        """

rule format_dada2:
    """
    This rule formats sequences from the vsearch clustered fasta into DADA2
    compatible formats.
    """
    message:
        "Generate DADA2 format fasta files"
    input:
        tsv=os.path.join(config["output_dir"], "coidb.info.tsv"),
        fasta=rules.add_to_vsearch.output.fasta,
        consensus=rules.calculate_consensus.output.tsv,
    output:
        to_genus=temp(os.path.join(config["output_dir"], "dada2/coidb.dada2.toGenus.fasta.gz")),
        to_species=temp(os.path.join(config["output_dir"], "dada2/coidb.dada2.toSpecies.fasta.gz")),
        assign_species=temp(os.path.join(config["output_dir"], "dada2/coidb.dada2.addSpecies.fasta.gz")),
    log:
        os.path.join(config["output_dir"], "_logs/format_dada2.log"),
    shell:
        """
        format --tsv {input.tsv} --fasta {input.fasta} --consensus {input.consensus} dada2 \
            --outfile_toGenus {output.to_genus} --outfile_toSpecies {output.to_species} \
            --outfile_assignSpecies {output.assign_species} 2>{log}
        """

rule format_qiime2:
    """
    This rule formats the tsv file for use with QIIME2 via
    qiime tools import --type 'FeatureData[Taxonomy]' --input-format TSVTaxonomyFormat
    """
    message:
        "Generate QIIME2 compatible TSV file"
    input:
        tsv=os.path.join(config["output_dir"], "coidb.info.tsv"),
        fasta=rules.add_to_vsearch.output.fasta,
        consensus=rules.calculate_consensus.output.tsv,
    output:
        os.path.join(config["output_dir"], "qiime2", "coidb.qiime2.info.tsv.gz")
    log:
        os.path.join(config["output_dir"], "_logs", "format_qiime2.log")
    shell:
        """
        format --tsv {input.tsv} --fasta {input.fasta} --consensus {input.consensus} qiime2 \
            --qiime2_outfile {output} 2>{log}
        """
             
rule concat_timestamps:
    """
    This rule combines the timestamp files into a single file.
    """
    output:
        os.path.join(config["output_dir"], "timestamps.txt")
    input:
        rules.download_backbone.output.timestamp,
        rules.filter.output.timestamp
    shell:
        """
        cat {input} > {output}
        """